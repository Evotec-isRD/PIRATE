{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d6733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import py3Dmol\n",
    "import subprocess\n",
    "import time\n",
    "import more_itertools as mit\n",
    "import itertools\n",
    "import tempfile\n",
    "import torch\n",
    "import biotite.structure as struc\n",
    "import biotite.structure.io as strucio\n",
    "import biotite.application.dssp as dssp\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.under_sampling import NearMiss, CondensedNearestNeighbour, RandomUnderSampler, InstanceHardnessThreshold, AllKNN\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "from scipy.stats import gmean\n",
    "from statsmodels.tsa.stattools import breakvar_heteroskedasticity_test\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_score, auc, make_scorer, recall_score, matthews_corrcoef, f1_score\n",
    "import optuna\n",
    "import catboost\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1498cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data, input_size):\n",
    "    \"\"\"\n",
    "    Takes in fasta sequence and returns encoded/padded data\n",
    "    \"\"\"\n",
    "    residue_dictionary = {\"A\": 1, \"E\": 2, \"L\": 3, \"M\": 4, \"C\": 5, \"D\": 6, \"F\": 7, \"G\": 8,\n",
    "                          \"H\": 9, \"K\":10, \"N\": 11, \"P\": 12, \"Q\": 13, \"R\": 14, \"S\": 15,\n",
    "                          \"W\": 16, \"Y\": 17, \"T\": 18, \"V\": 19, \"I\": 20}\n",
    "    \n",
    "    fasta = list(str(data))\n",
    "    # Encode data\n",
    "    for index, value in enumerate(fasta):\n",
    "        fasta[index] = residue_dictionary[value]\n",
    "    # Pad data\n",
    "\n",
    "    # Invert FASTA and make list 200 times the length to avoid edge cases where FASTA is small\n",
    "    padding = fasta[::-1]*2000\n",
    "    \n",
    "    split = int((input_size-len(fasta))/2)\n",
    "    last_padding_len = input_size - len(fasta) - split\n",
    "\n",
    "    stop_pos = int(split+len(fasta))\n",
    "    padding_1 = padding[-split:]\n",
    "    padding_2 = padding[:last_padding_len]\n",
    "    fasta = padding_1 + fasta + padding_2\n",
    "    \n",
    "    # Reshape data for input\n",
    "    fasta = np.array(fasta).reshape(-1, input_size, 1)\n",
    "    # Normalize data by subtracting training mean and dividing by training std. deviation\n",
    "    fasta = (fasta - 10.108613363425793)/6.034641898334733\n",
    "    return fasta, split, stop_pos\n",
    "\n",
    "def encode_presort_data(data, input_size):\n",
    "    \"\"\"\n",
    "    Takes in fasta sequence and returns encoded/padded data\n",
    "    \"\"\"\n",
    "    residue_dictionary = {\"A\": 1, \"E\": 2, \"L\": 3, \"M\": 4, \"C\": 5, \"D\": 6, \"F\": 7, \"G\": 8,\n",
    "                          \"H\": 9, \"K\":10, \"N\": 11, \"P\": 12, \"Q\": 13, \"R\": 14, \"S\": 15,\n",
    "                          \"W\": 16, \"Y\": 17, \"T\": 18, \"V\": 19, \"I\": 20}\n",
    "    \n",
    "    fasta = list(str(data))\n",
    "    # Encode data\n",
    "    for index, value in enumerate(fasta):\n",
    "        fasta[index] = residue_dictionary[value]\n",
    "    # Pad data\n",
    "\n",
    "    # Invert FASTA and make list 200 times the length to avoid edge cases where FASTA is small\n",
    "    padding = fasta[::-1]*2000\n",
    "    \n",
    "    split = int((input_size-len(fasta))/2)\n",
    "    last_padding_len = input_size - len(fasta) - split\n",
    "\n",
    "    stop_pos = int(split+len(fasta))\n",
    "    padding_1 = padding[-split:]\n",
    "    padding_2 = padding[:last_padding_len]\n",
    "    fasta = padding_1 + fasta + padding_2\n",
    "    \n",
    "    # Reshape data for input\n",
    "    fasta = np.array(fasta).reshape(-1, input_size, 1)\n",
    "    # Normalize data by subtracting training mean and dividing by training std. deviation\n",
    "    fasta = (fasta - 10.15)/5.98\n",
    "    return fasta, split, stop_pos\n",
    "\n",
    "\n",
    "def predict_data(fasta, model, input_size):\n",
    "    \"\"\"\n",
    "    Generate prediction for data point. Will return either probability of \n",
    "    crystallization or a classification.\n",
    "    \"\"\"\n",
    "\n",
    "    data, start_pos, stop_pos = encode_data(fasta, input_size)\n",
    "    prediction = model.predict(data).reshape(input_size, 1)\n",
    "    prediction = prediction[start_pos:stop_pos]\n",
    "    prediction = [float(i) for i in prediction]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def presort_data(fasta, model, input_size):\n",
    "    \"\"\"\n",
    "    Generate prediction for data point. Will return either probability of \n",
    "    crystallization or a classification.\n",
    "    \"\"\"\n",
    "\n",
    "    data, start_pos, stop_pos = encode_presort_data(fasta, input_size)\n",
    "    prediction = model.predict(data)[0]\n",
    "    prediction = list(prediction[:,1])\n",
    "    prediction = prediction[start_pos:stop_pos]\n",
    "    prediction = [float(i) for i in prediction]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def encode_sequence(fasta):\n",
    "    \n",
    "    residue_dictionary = {\"A\": 1, \"E\": 2, \"L\": 3, \"M\": 4, \"C\": 5, \"D\": 6, \"F\": 7, \"G\": 8,\n",
    "                          \"H\": 9, \"K\":10, \"N\": 11, \"P\": 12, \"Q\": 13, \"R\": 14, \"S\": 15,\n",
    "                          \"W\": 16, \"Y\": 17, \"T\": 18, \"V\": 19, \"I\": 20}\n",
    "    \n",
    "    fasta = list(str(fasta))\n",
    "    # Encode data\n",
    "    for index, value in enumerate(fasta):\n",
    "        fasta[index] = int(residue_dictionary[value])\n",
    "        \n",
    "    return fasta\n",
    "\n",
    "def process_protein(sequence, mae_pred, plddt_pred, presort_pred, ordinal_list, model):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    win_size = 11\n",
    "    \n",
    "    start, label, stop = 0, int(win_size), int((win_size * 2) + 1)\n",
    "    \n",
    "\n",
    "    while stop < len(sequence)+1:\n",
    "        \n",
    "        prediction = model.predict(mae_pred[start:stop] + plddt_pred[start:stop] + presort_pred[start:stop] + ordinal_list[start:stop])\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        start += 1\n",
    "        label += 1\n",
    "        stop += 1\n",
    "        \n",
    "    if predictions[0] == 0 and np.mean(np.array(presort_pred[:12])) < 0.7:\n",
    "        \n",
    "        predictions = [0]*win_size + predictions\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        predictions = [1]*win_size + predictions\n",
    "        \n",
    "    if predictions[-1] == 0 and np.mean(np.array(presort_pred[-12:])) < 0.7:\n",
    "        \n",
    "        predictions += [0]*win_size\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        predictions += [1]*win_size\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def fold_sequence(sequence):\n",
    "    \n",
    "    command = [\"curl\", \"-X\", \"POST\", \"--data\", \n",
    "           str(sequence), \n",
    "           \"https://api.esmatlas.com/foldSequence/v1/pdb/\"]\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    \n",
    "    return result.stdout\n",
    "\n",
    "def display_model_with_predictions(model, predictions):\n",
    "    \n",
    "    \n",
    "    colors = {}\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == 0:\n",
    "            colors[i+1] = \"red\"\n",
    "        else:\n",
    "            colors[i+1] = \"black\"\n",
    "    \n",
    "    p = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js')\n",
    "    p.addModel(model, 'pdb')\n",
    "    p.setStyle({'cartoon':{'colorscheme':{'prop':'resi','map':colors}}})\n",
    "    p.setHoverable({}, True, '''function(atom,viewer,event,container) {\n",
    "                            if(!atom.label) {\n",
    "                            atom.label = viewer.addLabel(atom.resn+\":\"+atom.resi,{position: atom, \n",
    "                            backgroundColor: 'mintcream', fontColor:'black'});\n",
    "                            }}''',\n",
    "                        '''function(atom,viewer) { \n",
    "                            if(atom.label) {\n",
    "                            viewer.removeLabel(atom.label);\n",
    "                            delete atom.label;\n",
    "                            }\n",
    "                            }''')\n",
    "    p.zoomTo()\n",
    "    p.show()\n",
    "    \n",
    "    \n",
    "def display_model(model):\n",
    "    \n",
    "    \n",
    "    p = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js')\n",
    "    p.addModel(model, 'pdb')\n",
    "    p.setStyle({'cartoon': {'color': 'spectrum', 'colorscheme': 'roygb'}})\n",
    "    p.setHoverable({}, True, '''function(atom,viewer,event,container) {\n",
    "                            if(!atom.label) {\n",
    "                            atom.label = viewer.addLabel(atom.resn+\":\"+atom.resi,{position: atom, \n",
    "                            backgroundColor: 'mintcream', fontColor:'black'});\n",
    "                            }}''',\n",
    "                        '''function(atom,viewer) { \n",
    "                            if(atom.label) {\n",
    "                            viewer.removeLabel(atom.label);\n",
    "                            delete atom.label;\n",
    "                            }\n",
    "                            }''')\n",
    "    p.zoomTo()\n",
    "    p.show()\n",
    "    \n",
    "def get_mean_b_factor(model):\n",
    "    \n",
    "    model_list = model.split(\"\\n\")\n",
    "    b_factors = []\n",
    "    for count,e in enumerate(model_list):\n",
    "        if count > 21:\n",
    "            b_factors.append(float(e[e.index(\"1.00  \")+6:e.index(\"1.00  \")+10]))\n",
    "            \n",
    "    return float(np.mean(np.array(b_factors))) \n",
    "\n",
    "def get_min_b_factor(model):\n",
    "    \n",
    "    model_list = model.split(\"\\n\")\n",
    "    b_factors = []\n",
    "    for count,e in enumerate(model_list):\n",
    "        if count > 21:\n",
    "            b_factors.append(float(e[e.index(\"1.00  \")+6:e.index(\"1.00  \")+10]))\n",
    "            \n",
    "    return float(np.min(np.array(b_factors)))\n",
    "    \n",
    "def remove_predictions(sequence, predictions):\n",
    "    \n",
    "    sequence = bytearray(sequence, encoding='utf8')\n",
    "    for count, i in reversed(list(enumerate(predictions))):\n",
    "        \n",
    "        if i == 0:\n",
    "            del sequence[count]\n",
    "            \n",
    "    return str(sequence.decode())\n",
    "\n",
    "def disorder_probability(sequence: str) -> float:\n",
    "\n",
    "    predictions = []\n",
    "    # generate encodings for sequence\n",
    "    mae_pred = predict_data(sequence, e3p_mae_model, 4096)\n",
    "    plddt_pred = list(np.array(predict_data(sequence, e3p_plddt_model, 4096))/100.0)\n",
    "    presort_pred = presort_data(sequence, presort_model, 2048)\n",
    "    ordinal_list = encode_sequence(sequence)\n",
    "    # window size of predictions\n",
    "    win_size = 11\n",
    "\n",
    "    start, label, stop = 0, int(win_size), int((win_size * 2) + 1)\n",
    "\n",
    "    while stop < len(sequence) + 1:\n",
    "        prediction = cat_model.predict_proba(\n",
    "            mae_pred[start:stop] + plddt_pred[start:stop] + presort_pred[start:stop] +\n",
    "        ordinal_list[start:stop])[0]\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        start += 1\n",
    "        label += 1\n",
    "        stop += 1\n",
    "\n",
    "\n",
    "    mean_proba = float(np.mean(np.array(predictions)))\n",
    "\n",
    "    return mean_proba\n",
    "\n",
    "def disorder_list(sequence: str) -> float:\n",
    "\n",
    "    predictions = []\n",
    "    # generate encodings for sequence\n",
    "    mae_pred = predict_data(sequence, e3p_mae_model, 4096)\n",
    "    plddt_pred = list(np.array(predict_data(sequence, e3p_plddt_model, 4096))/100.0)\n",
    "    presort_pred = presort_data(sequence, presort_model, 2048)\n",
    "    ordinal_list = encode_sequence(sequence)\n",
    "    # window size of predictions\n",
    "    win_size = 11\n",
    "\n",
    "    start, label, stop = 0, int(win_size), int((win_size * 2) + 1)\n",
    "\n",
    "    while stop < len(sequence) + 1:\n",
    "        prediction = cat_model.predict_proba(\n",
    "            mae_pred[start:stop] + plddt_pred[start:stop] + presort_pred[start:stop] +\n",
    "        ordinal_list[start:stop])[0]\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        start += 1\n",
    "        label += 1\n",
    "        stop += 1\n",
    "\n",
    "    predictions = [0]*win_size + predictions\n",
    "    predictions = predictions + [0]*win_size\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def plot_disorder(sequence, name):\n",
    "    \"\"\"\n",
    "    Function that plots the disorder redictions for each residue as a time-series\n",
    "    \"\"\"\n",
    "    predictions = disorder_list(sequence)\n",
    "    arr = np.array(predictions)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    plt.title(f\"Probability of Disorder per Residue for {name}\",loc=\"center\")\n",
    "    plt.xlabel(\"Residue\")\n",
    "    plt.ylabel(\"Disorder Probability\")\n",
    "    ax.plot(arr)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def normalize_list(input_list):\n",
    "\n",
    "    input_list = np.array(input_list)\n",
    "    min = np.min(input_list)+0.0001\n",
    "    max = np.max(input_list)\n",
    "\n",
    "    normalized_list = (input_list - min) / (max - min)\n",
    "    normalized_list = normalized_list.tolist()\n",
    "\n",
    "    return normalized_list\n",
    "\n",
    "def get_evo_probabilities(sequence):\n",
    "\n",
    "    evo_probabilities = []\n",
    "    evo_model = \"esm1v_t33_650M_UR90S_1.pt\"\n",
    "    model, alphabet = pretrained.load_model_and_alphabet(evo_model)\n",
    "\n",
    "    data = [(\"sequence\", sequence),]\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    token_probs = torch.log_softmax(model(batch_tokens)[\"logits\"], dim=-1)\n",
    "\n",
    "    for count, residue in enumerate(sequence):\n",
    "\n",
    "        wt_encoded, mt_encoded = alphabet.get_idx(residue), alphabet.get_idx(\"R\")\n",
    "        score = token_probs[0, 1 + count, mt_encoded] - token_probs[0, 1 + count, wt_encoded]\n",
    "        evo_probabilities.append(score.detach().numpy())\n",
    "\n",
    "    for count, e in enumerate(evo_probabilities):\n",
    "        evo_probabilities[count] = math.exp(e)\n",
    "    \n",
    "    return evo_probabilities\n",
    "\n",
    "def get_sasa(sequence):\n",
    "\n",
    "    model = fold_sequence(sequence)\n",
    "    with open ('test.pdb', 'w') as file:  \n",
    "        for line in model:  \n",
    "            file.write(line)  \n",
    "    array = strucio.load_structure(\"test.pdb\")\n",
    "    atom_sasa = struc.sasa(array, vdw_radii=\"Single\")\n",
    "    res_sasa = struc.apply_residue_wise(array, atom_sasa, np.sum)\n",
    "    res_sasa = res_sasa.tolist()\n",
    "\n",
    "    return res_sasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b240aec7-2924-4d1c-8ef4-68b19397352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae loaded\n",
      "plddt loaded\n",
      "presort loaded\n",
      "catboost loaded\n"
     ]
    }
   ],
   "source": [
    "e3p_mae_path = r\"C:\\Users\\GRICHARDSON\\OneDrive - Evotec\\Desktop\\crystallization_deletion_tool\\e3p_mae\"\n",
    "e3p_plddt_path = r\"C:\\Users\\GRICHARDSON\\OneDrive - Evotec\\Desktop\\crystallization_deletion_tool\\e3p_plddt\"\n",
    "presort_path = r\"C:\\Users\\GRICHARDSON\\OneDrive - Evotec\\Desktop\\crystallization_deletion_tool\\presort\"\n",
    "catboost_path = r\"C:\\Users\\GRICHARDSON\\OneDrive - Evotec\\Desktop\\crystallization_deletion_tool\\catboost_model_win11_all4_allknn_undersampled.pkl\"\n",
    "input_size = 4096\n",
    "presort_input = 2048\n",
    "e3p_mae_model = tf.keras.models.load_model(e3p_mae_path, custom_objects=None, compile=True, options=None)\n",
    "print(\"mae loaded\")\n",
    "e3p_plddt_model = tf.keras.models.load_model(e3p_plddt_path, custom_objects=None, compile=True, options=None)\n",
    "print(\"plddt loaded\")\n",
    "presort_model = tf.keras.models.load_model(presort_path, custom_objects=None, compile=True, options=None)\n",
    "print(\"presort loaded\")\n",
    "cat_model = pickle.load(open(catboost_path, 'rb'))\n",
    "print(\"catboost loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982d951c-01a9-418d-8534-62747d19dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from itertools import groupby\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb757be-6eac-47af-aa76-da6ae6190c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GRICHARDSON\\AppData\\Local\\miniforge3\\envs\\oss\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "checkpoint = r\"C:\\Users\\GRICHARDSON\\OneDrive - Evotec\\Desktop\\crystallization_deletion_tool\\DR-BERT-final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "bert_model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "bert_model = bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b11672-6ec5-473b-b1d7-01e53a7666e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >sp|P20711|DDC_HUMAN Aromatic-L-amino-acid decarboxylase OS=Homo sapiens OX=9606 GN=DDC PE=1 SV=2\n",
    "aadc = \"MNASEFRRRGKEMVDYMANYMEGIEGRQVYPDVEPGYLRPLIPAAAPQEPDTFEDIINDVEKIIMPGVTHWHSPYFFAYFPTASSYPAMLADMLCGAIGCIGFSWAASPACTELETVMMDWLGKMLELPKAFLNEKAGEGGGVIQGSASEATLVALLAARTKVIHRLQAASPELTQAAIMEKLVAYSSDQAHSSVERAGLIGGVKLKAIPSDGNFAMRASALQEALERDKAAGLIPFFMVATLGTTTCCSFDNLLEVGPICNKEDIWLHVDAAYAGSAFICPEFRHLLNGVEFADSFNFNPHKWLLVNFDCSAMWVKKRTDLTGAFRLDPTYLKHSHQDSGLITDYRHWQIPLGRRFRSLKMWFVFRMYGVKGLQAYIRKHVQLSHEFESLVRQDPRFEICVEVILGLVCFRLKGSNKVNEALLQRINSAKKIHLVPCHLRDKFVLRFAICSRTVESAHVQRAWEHIKELAADVLRAERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed49593-7e80-4eeb-b487-e29ba8fc3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >sp|P68104|EF1A1_HUMAN Elongation factor 1-alpha 1 OS=Homo sapiens OX=9606 GN=EEF1A1 PE=1 SV=1\n",
    "ef1a = \"MGKEKTHINIVVIGHVDSGKSTTTGHLIYKCGGIDKRTIEKFEKEAAEMGKGSFKYAWVLDKLKAERERGITIDISLWKFETSKYYVTIIDAPGHRDFIKNMITGTSQADCAVLIVAAGVGEFEAGISKNGQTREHALLAYTLGVKQLIVGVNKMDSTEPPYSQKRYEEIVKEVSTYIKKIGYNPDTVAFVPISGWNGDNMLEPSANMPWFKGWKVTRKDGNASGTTLLEALDCILPPTRPTDKPLRLPLQDVYKIGGIGTVPVGRVETGVLKPGMVVTFAPVNVTTEVKSVEMHHEALSEALPGDNVGFNVKNVSVKDVRRGNVAGDSKNDPPMEAAGFTAQVIILNHPGQISAGYAPVLDCHTAHIACKFAELKEKIDRRSGKKLEDGPKFLKSGDAAIVDMVPGKPMCVESFSDYPPLGRFAVRDMRQTVAVGVIKAVDKKAAGAGKVTKSAQKAQKAK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4b0921-d1c4-4023-af44-9d18544a9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [aadc, ef1a]\n",
    "sequence_ids = [\"aadc\", \"ef1a\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d646318b-1b40-4b3f-b615-5a58a59d89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GRICHARDSON\\AppData\\Local\\Temp\\ipykernel_15016\\1006409631.py:73: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  prediction = [float(i) for i in prediction]\n"
     ]
    }
   ],
   "source": [
    "pirate_list = []\n",
    "for sequence in sequences:\n",
    "    pirate_list.append(disorder_list(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32352db8-ab98-4bf3-bc39-097cc7c7e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GRICHARDSON\\AppData\\Local\\Temp\\ipykernel_15016\\2193465111.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(torch.squeeze(output['logits']))[2:-2,1].detach().numpy().tolist()\n"
     ]
    }
   ],
   "source": [
    "bert_list = []\n",
    "for sequence in sequences:\n",
    "    encoded = tokenizer.encode_plus((\"something\", str(sequence)), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(**encoded)\n",
    "    output = F.softmax(torch.squeeze(output['logits']))[2:-2,1].detach().numpy().tolist()\n",
    "    bert_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ed09c5-7f25-443e-b19c-4e6f68555444",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data[\"sequence_ids\"] = sequence_ids\n",
    "data[\"pirate_preds\"] = pirate_list\n",
    "data[\"dr_bert_preds\"] = bert_list\n",
    "\n",
    "data.to_csv(\"aadc_efa1_preds_pirate_bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192396f-92d9-4204-8ff6-a18f5baf6730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
